# cuda最优策略

衡量标准为每个流处理器（SM）的占用率

## 1.共享内存大小

帕斯卡 ( Pascal ) 64k

麦克斯韦尔 ( Maxwell ) 96k

## 2.寄存器个数

cuda中寄存器用于记录核函数使用时的变量个数，单个线程最大为为255.

但是启动的总线程数*变量数不得超过每个块的上限（64k）

## 3.线程/块的大小

线程少块多，线程多块少。每个SM上限32块。

## 4.线程束的个数

每个SM最多64个线程束。衡量占用率的最终决定因素。

## cuda计算原理

![image-20210609141320557](.\图片\gpu架构图.png)

# 暗通道加速项目

难点

## 内存缓存设计和边缘处理。

按照行为单元进行计算。

## bit排序

将任意一个长为2n的双调序列A分为等长的两半X和Y，将X中的元素与Y中的元素一一按原序比较，即a[i]与a[i+n] (i < n)比较，将较大者放入MAX序列，较小者放入MIN序列。则得到的MAX和MIN序列仍然是双调序列，并且MAX序列中的任意一个元素不小于MIN序列中的任意一个元素[2]。

# GPU硬件架构介绍：

GPU的核心数量不同于CPU核心，GPU核心往往具有成百上千个核心，为此为了管理便于管理核心，GPU硬件上存在若干被称为**流式处理器SM**的硬件结构用于管理GPU核心的调度。每个SM管理32个GPU核心，具有一片独立的L1内存。能够同时为16个核心提供数据。

同时GPU存在一种名为**千兆调度器**的硬件结构，这个结构将决定我们假想的线程任务具体被落实到哪个SM来执行。

**高速共享内存L2**：这是GPU内部除了显存以外唯一的一片共享内存，对于我项目来说大概为96KB。

**内存控制器：**将显卡全局显存载入L2；

以上为简要的硬件基本情况介绍；

GPU加速流程：CPU端将内存内数据通过对应接口拷贝到GPU对应显存，然后通过**内存控制器**提供的接口将全局显存上的需要高效计算数据的数据拷贝到**高速共享内存L2**中，之后**L1控制器**将自动将数据传输到对应的GPU核心中，等上述过程就绪之后。**千兆调度器**将会负责将项目程序中编写的虚拟线程去寻找物理意义上空闲的**流式处理器SM**。最终实现计算。

cuda程序的难点：

软件层面上，所有的任务以块为单位进行执行，每个块内所有的线程执行的程序都相同。单个块可以执行32-1024个线程。而每个SM最多能启动32个块共2048个线程。这就需要进行合理的分配，尽量减少无用的碎片线程产生。

需要进行**L2高速共享内存**区域的内存手工管理，所有SM都共享这片区域如果单个SM所占用过多会导致实际能够运行的SM数量减少。

同时每个具体线程所执行的函数我们称为核函数，每个线程最多只能拥有255个变量。同时每个块所能有变量个数也有上限。所以跟之前共享内存的道理一样需要合理分配。

综合以上多种角度，最终绝对效率的因素为，SM的线程利用率，达到2048线程全利用即为效率最高的设计。

# 项目socket：

客户端（上位机）

![image-20210719104515660](图片\客户端socket.png)

服务器端（上位机）

![image-20210719104616903](图片\服务器端socket)

# 相机初始化：

设计了一个类，里面包括一些自己整合的设置曝光增益等接口dhcam，在保留原相机接口抛出异常的功能基础上简单解决一些错误，例如数据越界。

同时类里面还有一个类用于存放相机的各种参数和相机获取数据的临时的存放，这个类可以用于其他相机camcap。

作用：将设置参数等需要一系列参数的复杂原始接口，封装为给参数就能用的简单模式，也省去了一步一个异常检测的麻烦。



# GPU图像处理时的像素序列到线程序列：

主要写了一个滤波类模板：

一个二维的grid，x为一行像素要用的block数，y为图像行数。

每个block会浪费2*滤波半径个线程

统一先计算列半径最小值

在计算行半径。

优点通过线程和行数的映射，避免了一次坐标计算。

